{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hieuhuynh/Apps/anaconda3/envs/text-cls-py3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "import text_cnn\n",
    "import data_helper\n",
    "from utils import parse_reuters, remove_minor_classes, topics_with_occurences_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file ../data/reut2-000.sgm\n",
      "processing file ../data/reut2-001.sgm\n",
      "processing file ../data/reut2-002.sgm\n",
      "processing file ../data/reut2-003.sgm\n",
      "processing file ../data/reut2-004.sgm\n",
      "processing file ../data/reut2-005.sgm\n",
      "processing file ../data/reut2-006.sgm\n",
      "processing file ../data/reut2-007.sgm\n",
      "processing file ../data/reut2-008.sgm\n",
      "processing file ../data/reut2-009.sgm\n",
      "processing file ../data/reut2-010.sgm\n",
      "processing file ../data/reut2-011.sgm\n",
      "processing file ../data/reut2-012.sgm\n",
      "processing file ../data/reut2-013.sgm\n",
      "processing file ../data/reut2-014.sgm\n",
      "processing file ../data/reut2-015.sgm\n",
      "processing file ../data/reut2-016.sgm\n",
      "processing file ../data/reut2-017.sgm\n",
      "processing file ../data/reut2-018.sgm\n",
      "processing file ../data/reut2-019.sgm\n",
      "processing file ../data/reut2-020.sgm\n",
      "processing file ../data/reut2-021.sgm\n",
      "Removed 58 topics with occurrences greater than 3. remain: 77 topics\n",
      "Removing samples with minor topics\n",
      "Train samples: 7773\n",
      "Test samples: 3016\n"
     ]
    }
   ],
   "source": [
    "_train_text_lst, _train_label_lst, _test_text_lst, _test_label_lst = parse_reuters()\n",
    "\n",
    "# remove classes with very few occurrences\n",
    "topics_selected = topics_with_occurences_gt(3, _train_label_lst)\n",
    "train_text_lst, train_label_lst, test_text_lst, test_label_lst = remove_minor_classes(topics_selected, _train_text_lst, _train_label_lst, _test_text_lst, _test_label_lst)\n",
    "\n",
    "# label transformer\n",
    "label_binarizer = preprocessing.MultiLabelBinarizer(topics_selected)\n",
    "label_binarizer.fit(train_label_lst)\n",
    "y_train = label_binarizer.transform(train_label_lst)\n",
    "y_test = label_binarizer.transform(test_label_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length of all sentences: 1267\n",
      "WARNING:tensorflow:From <ipython-input-7-84ee151a9df8>:7: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /Users/hieuhuynh/Apps/anaconda3/envs/text-cls-py3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /Users/hieuhuynh/Apps/anaconda3/envs/text-cls-py3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "x train shape: (7773, 1267)\n",
      "x test shape: (3016, 1267)\n"
     ]
    }
   ],
   "source": [
    "# step 1\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "max_document_length = max([len(x.split(' ')) for x in train_text_lst])\n",
    "print('The maximum length of all sentences: {}'.format(max_document_length))\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x_train = np.array(list(vocab_processor.fit_transform(train_text_lst)))\n",
    "x_test = np.array(list(vocab_processor.transform(test_text_lst)))\n",
    "print('x train shape: {}'.format(x_train.shape))\n",
    "print('x test shape: {}'.format(x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store data as pkl files\n",
    "import pickle as pkl\n",
    "with open('x_train.pkl', 'wb') as xtf:\n",
    "    pkl.dump(x_train, xtf)\n",
    "    xtf.close()\n",
    "with open('x_test.pkl', 'wb') as xtesf:\n",
    "    pkl.dump(x_test, xtesf)\n",
    "    xtesf.close()\n",
    "with open('y_train.pkl', 'wb') as ytf:\n",
    "    pkl.dump(y_train, ytf)\n",
    "    ytf.close()\n",
    "with open('y_test.pkl', 'wb') as ytesf:\n",
    "    pkl.dump(y_test, ytesf)\n",
    "    ytesf.close()\n",
    "\n",
    "with open('topics_selected.pkl', 'wb') as tselectedf:\n",
    "    pkl.dump(topics_selected, tselectedf)\n",
    "    tselectedf.close()\n",
    "# Save the word_to_id map since predict.py needs it\n",
    "vocab_processor.save(os.path.join(os.getcwd(), \"vocab.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train shape: (7773, 1267)\n",
      "x test shape: (3016, 1267)\n",
      "y train shape: (7773, 77)\n",
      "y test shape: (3016, 77)\n"
     ]
    }
   ],
   "source": [
    "# load data from pkl files\n",
    "import pickle as pkl\n",
    "with open('x_train.pkl', 'rb') as xtf:\n",
    "    x_train = pkl.load(xtf)\n",
    "    xtf.close()\n",
    "with open('x_test.pkl', 'rb') as xtesf:\n",
    "    x_test = pkl.load(xtesf)\n",
    "    xtesf.close()\n",
    "with open('y_train.pkl', 'rb') as ytf:\n",
    "    y_train = pkl.load(ytf)\n",
    "    ytf.close()\n",
    "with open('y_test.pkl', 'rb') as ytesf:\n",
    "    y_test = pkl.load(ytesf)\n",
    "    ytesf.close()\n",
    "    \n",
    "with open('topics_selected.pkl', 'rb') as tselectedf:\n",
    "    topics_selected = pkl.load(tselectedf)\n",
    "    tselectedf.close()\n",
    "\n",
    "with open('vocab.pickle', 'rb') as vocabf:\n",
    "    vocab_processor = pkl.load(vocabf)\n",
    "    vocabf.close()\n",
    "print('x train shape: {}'.format(x_train.shape))\n",
    "print('x test shape: {}'.format(x_test.shape))\n",
    "print('y train shape: {}'.format(y_train.shape))\n",
    "print('y test shape: {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y_train)))\n",
    "x_shuffled = x_train[shuffle_indices]\n",
    "y_shuffled = y_train[shuffle_indices]\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(x_shuffled, y_shuffled, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "sess = tf.Session(config=session_conf)\n",
    "cnn = text_cnn.TextCNN(\n",
    "    sequence_length=x_train.shape[1],\n",
    "    num_classes=y_train.shape[1],\n",
    "    vocab_size=len(vocab_processor.vocabulary_),\n",
    "    embedding_size=50,\n",
    "    filter_sizes=[3, 4, 5],\n",
    "    num_filters=32,\n",
    "    l2_reg_lambda=0.0)\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(learning_rate=1e-3,\n",
    "                                           global_step=global_step, \n",
    "                                           decay_steps=300,\n",
    "                                           decay_rate=0.95, staircase=True)\n",
    "optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_step(x_batch, y_batch):\n",
    "    feed_dict = {\n",
    "        cnn.input_x: x_batch,\n",
    "        cnn.input_y: y_batch,\n",
    "        cnn.dropout_keep_prob: 0.5}\n",
    "    _, step, loss = sess.run([train_op, global_step, cnn.loss], feed_dict)\n",
    "    return loss\n",
    "# One evaluation step: evaluate the model with one batch\n",
    "def dev_step(x_batch, y_batch):\n",
    "    feed_dict = {cnn.input_x: x_batch, cnn.input_y: y_batch, cnn.dropout_keep_prob: 1.0}\n",
    "    step, scores, loss = sess.run([global_step, cnn.scores, cnn.loss], feed_dict)\n",
    "    predicted_labels_threshold, predicted_values_threshold, y_batch_pred = data_helper.get_label_using_scores_by_threshold(scores=scores, threshold=0.5)\n",
    "    \n",
    "    cur_rec_ts, cur_acc_ts, cur_F_ts = 0.0, 0.0, 0.0\n",
    "    \n",
    "    for index, predicted_label_threshold in enumerate(predicted_labels_threshold):\n",
    "        rec_inc_ts, acc_inc_ts = data_helper.cal_metric(predicted_label_threshold, y_batch[index])\n",
    "        cur_rec_ts, cur_acc_ts = cur_rec_ts + rec_inc_ts, cur_acc_ts + acc_inc_ts\n",
    "        \n",
    "    cur_rec_ts = cur_rec_ts / len(y_batch)\n",
    "    cur_acc_ts = cur_acc_ts / len(y_batch)\n",
    "    cur_F_ts = data_helper.cal_F(cur_rec_ts, cur_acc_ts)\n",
    "    return cur_rec_ts, cur_acc_ts, cur_F_ts, loss, y_batch_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Step 6: train the cnn model with x_train and y_train (batch by batch)\n",
    "batch_size = 32\n",
    "num_epochs = 30\n",
    "train_batches = data_helper.batch_iter(list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "best_accuracy, best_at_step = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train step 50 loss: 7.9856\n",
      "Train step 100 loss: 6.9490\n",
      "Train step 150 loss: 5.3841\n",
      "Train step 200 loss: 4.5441\n",
      "---------- Evaluate on step 200 -----------\n",
      "loss on dev set: 4.658696460723877\n",
      "recall on dev set: 0.45379166666666665\n",
      "accuracy on dev set: 0.46\n",
      "F score on dev set: 0.4568747435137477\n",
      " Epoch 1 has finished!\n",
      "Train step 250 loss: 5.1319\n",
      "Train step 300 loss: 5.0333\n",
      "Train step 350 loss: 3.0060\n",
      "Train step 400 loss: 5.0801\n",
      "---------- Evaluate on step 400 -----------\n",
      "loss on dev set: 4.310470924377442\n",
      "recall on dev set: 0.48595833333333327\n",
      "accuracy on dev set: 0.49424999999999997\n",
      "F score on dev set: 0.49006909670563226\n",
      " Epoch 2 has finished!\n",
      "Train step 450 loss: 5.3404\n",
      "Train step 500 loss: 3.2028\n",
      "Saved model to checkpoint/model-500\n",
      "Train step 550 loss: 4.4943\n",
      "Train step 600 loss: 4.7353\n",
      "---------- Evaluate on step 600 -----------\n",
      "loss on dev set: 4.035959920883179\n",
      "recall on dev set: 0.5017916666666666\n",
      "accuracy on dev set: 0.50925\n",
      "F score on dev set: 0.5054933237172883\n",
      "Train step 650 loss: 3.7087\n",
      " Epoch 3 has finished!\n",
      "Train step 700 loss: 4.1894\n",
      "Train step 750 loss: 2.6141\n",
      "Train step 800 loss: 4.2881\n",
      "---------- Evaluate on step 800 -----------\n",
      "loss on dev set: 3.830132551193237\n",
      "recall on dev set: 0.5098125\n",
      "accuracy on dev set: 0.5227499999999999\n",
      "F score on dev set: 0.516200199745778\n",
      "Train step 850 loss: 4.0331\n",
      " Epoch 4 has finished!\n",
      "Train step 900 loss: 2.7217\n",
      "Train step 950 loss: 4.1379\n",
      "Train step 1000 loss: 3.4502\n",
      "---------- Evaluate on step 1000 -----------\n",
      "loss on dev set: 3.6391598892211916\n",
      "recall on dev set: 0.5170833333333333\n",
      "accuracy on dev set: 0.5457500000000001\n",
      "F score on dev set: 0.5310300689979615\n",
      "Saved model to checkpoint/model-1000\n",
      "Train step 1050 loss: 3.5216\n",
      " Epoch 5 has finished!\n",
      "Train step 1100 loss: 4.6999\n",
      "Train step 1150 loss: 2.8861\n",
      "Train step 1200 loss: 3.2998\n",
      "---------- Evaluate on step 1200 -----------\n",
      "loss on dev set: 3.3955057048797608\n",
      "recall on dev set: 0.5392916666666666\n",
      "accuracy on dev set: 0.57275\n",
      "F score on dev set: 0.5555174978455544\n",
      "Train step 1250 loss: 3.3153\n",
      "Train step 1300 loss: 3.1883\n",
      " Epoch 6 has finished!\n",
      "Train step 1350 loss: 2.8759\n",
      "Train step 1400 loss: 3.2490\n",
      "---------- Evaluate on step 1400 -----------\n",
      "loss on dev set: 3.255251617431641\n",
      "recall on dev set: 0.5796458333333332\n",
      "accuracy on dev set: 0.629\n",
      "F score on dev set: 0.6033152460570541\n",
      "Train step 1450 loss: 3.7382\n",
      "Train step 1500 loss: 3.7517\n",
      "Saved model to checkpoint/model-1500\n",
      " Epoch 7 has finished!\n",
      "Train step 1550 loss: 2.9326\n",
      "Train step 1600 loss: 2.8086\n",
      "---------- Evaluate on step 1600 -----------\n",
      "loss on dev set: 3.009103832244873\n",
      "recall on dev set: 0.5994375\n",
      "accuracy on dev set: 0.653\n",
      "F score on dev set: 0.6250734068566295\n",
      "Train step 1650 loss: 3.1390\n",
      "Train step 1700 loss: 3.3384\n",
      "Train step 1750 loss: 2.5703\n",
      " Epoch 8 has finished!\n",
      "Train step 1800 loss: 2.9637\n",
      "---------- Evaluate on step 1800 -----------\n",
      "loss on dev set: 2.893246917724609\n",
      "recall on dev set: 0.6178541666666667\n",
      "accuracy on dev set: 0.6677500000000001\n",
      "F score on dev set: 0.6418338248877797\n",
      "Train step 1850 loss: 1.8063\n",
      "Train step 1900 loss: 2.4660\n",
      "Train step 1950 loss: 2.4096\n",
      " Epoch 9 has finished!\n",
      "Train step 2000 loss: 1.9860\n",
      "---------- Evaluate on step 2000 -----------\n",
      "loss on dev set: 2.773989896774292\n",
      "recall on dev set: 0.6525624999999999\n",
      "accuracy on dev set: 0.70675\n",
      "F score on dev set: 0.6785761874109154\n",
      "Saved model to checkpoint/model-2000\n",
      "Train step 2050 loss: 2.7507\n",
      "Train step 2100 loss: 3.0167\n",
      "Train step 2150 loss: 2.3018\n",
      " Epoch 10 has finished!\n",
      "Train step 2200 loss: 2.1183\n",
      "---------- Evaluate on step 2200 -----------\n",
      "loss on dev set: 2.6934134721755982\n",
      "recall on dev set: 0.6599375\n",
      "accuracy on dev set: 0.7140000000000001\n",
      "F score on dev set: 0.6859051084929264\n",
      "Train step 2250 loss: 2.8774\n",
      "Train step 2300 loss: 1.8800\n",
      "Train step 2350 loss: 1.8374\n",
      "Train step 2400 loss: 3.4790\n",
      "---------- Evaluate on step 2400 -----------\n",
      "loss on dev set: 2.579038071632385\n",
      "recall on dev set: 0.6635416666666666\n",
      "accuracy on dev set: 0.72125\n",
      "F score on dev set: 0.6911933955167744\n",
      " Epoch 11 has finished!\n",
      "Train step 2450 loss: 2.1359\n",
      "Train step 2500 loss: 3.2621\n",
      "Saved model to checkpoint/model-2500\n",
      "Train step 2550 loss: 4.2551\n",
      "Train step 2600 loss: 2.1594\n",
      "---------- Evaluate on step 2600 -----------\n",
      "loss on dev set: 2.48167959690094\n",
      "recall on dev set: 0.6867708333333333\n",
      "accuracy on dev set: 0.748\n",
      "F score on dev set: 0.7160789324659862\n",
      " Epoch 12 has finished!\n",
      "Train step 2650 loss: 1.4670\n",
      "Train step 2700 loss: 1.9563\n",
      "Train step 2750 loss: 2.8027\n",
      "Train step 2800 loss: 1.9874\n",
      "---------- Evaluate on step 2800 -----------\n",
      "loss on dev set: 2.373795733451843\n",
      "recall on dev set: 0.7057083333333333\n",
      "accuracy on dev set: 0.7647499999999999\n",
      "F score on dev set: 0.734043849706724\n",
      " Epoch 13 has finished!\n",
      "Train step 2850 loss: 2.2165\n",
      "Train step 2900 loss: 2.1698\n",
      "Train step 2950 loss: 1.2579\n",
      "Train step 3000 loss: 2.6650\n",
      "---------- Evaluate on step 3000 -----------\n",
      "loss on dev set: 2.2654876267910002\n",
      "recall on dev set: 0.7148333333333333\n",
      "accuracy on dev set: 0.772875\n",
      "F score on dev set: 0.7427219437053634\n",
      "Saved model to checkpoint/model-3000\n",
      "Train step 3050 loss: 2.3844\n",
      " Epoch 14 has finished!\n",
      "Train step 3100 loss: 1.0852\n",
      "Train step 3150 loss: 1.7128\n",
      "Train step 3200 loss: 3.0244\n",
      "---------- Evaluate on step 3200 -----------\n",
      "loss on dev set: 2.3439434003829955\n",
      "recall on dev set: 0.7106875000000001\n",
      "accuracy on dev set: 0.773\n",
      "F score on dev set: 0.7405352373730991\n",
      "Train step 3250 loss: 2.2662\n",
      " Epoch 15 has finished!\n",
      "Train step 3300 loss: 2.0470\n",
      "Train step 3350 loss: 1.7931\n",
      "Train step 3400 loss: 1.7138\n",
      "---------- Evaluate on step 3400 -----------\n",
      "loss on dev set: 2.205084090232849\n",
      "recall on dev set: 0.7230833333333332\n",
      "accuracy on dev set: 0.7829999999999999\n",
      "F score on dev set: 0.7518498312399712\n",
      "Train step 3450 loss: 1.9378\n",
      "Train step 3500 loss: 3.2800\n",
      "Saved model to checkpoint/model-3500\n",
      " Epoch 16 has finished!\n",
      "Train step 3550 loss: 2.4608\n",
      "Train step 3600 loss: 1.7938\n",
      "---------- Evaluate on step 3600 -----------\n",
      "loss on dev set: 2.2228426933288574\n",
      "recall on dev set: 0.7183750000000001\n",
      "accuracy on dev set: 0.78175\n",
      "F score on dev set: 0.7487238146821098\n",
      "Train step 3650 loss: 2.7380\n",
      "Train step 3700 loss: 2.7245\n",
      " Epoch 17 has finished!\n",
      "Train step 3750 loss: 1.5970\n",
      "Train step 3800 loss: 2.4756\n",
      "---------- Evaluate on step 3800 -----------\n",
      "loss on dev set: 2.179374771118164\n",
      "recall on dev set: 0.7256041666666665\n",
      "accuracy on dev set: 0.78825\n",
      "F score on dev set: 0.7556308883231265\n",
      "Train step 3850 loss: 2.5049\n",
      "Train step 3900 loss: 1.5116\n",
      " Epoch 18 has finished!\n",
      "Train step 3950 loss: 3.1847\n",
      "Train step 4000 loss: 1.5123\n",
      "---------- Evaluate on step 4000 -----------\n",
      "loss on dev set: 2.137105164527893\n",
      "recall on dev set: 0.7385416666666668\n",
      "accuracy on dev set: 0.8007500000000001\n",
      "F score on dev set: 0.7683888016674338\n",
      "Saved model to checkpoint/model-4000\n",
      "Train step 4050 loss: 2.5345\n",
      "Train step 4100 loss: 2.1542\n",
      "Train step 4150 loss: 2.4521\n",
      " Epoch 19 has finished!\n",
      "Train step 4200 loss: 1.8455\n",
      "---------- Evaluate on step 4200 -----------\n",
      "loss on dev set: 2.091292462348938\n",
      "recall on dev set: 0.7487916666666663\n",
      "accuracy on dev set: 0.8114583333333334\n",
      "F score on dev set: 0.7788665122220441\n",
      "Train step 4250 loss: 1.7458\n",
      "Train step 4300 loss: 1.9074\n",
      "Train step 4350 loss: 1.9778\n",
      " Epoch 20 has finished!\n",
      "Train step 4400 loss: 1.7862\n",
      "---------- Evaluate on step 4400 -----------\n",
      "loss on dev set: 2.1094015884399413\n",
      "recall on dev set: 0.7406458333333333\n",
      "accuracy on dev set: 0.80475\n",
      "F score on dev set: 0.771368372450424\n",
      "Train step 4450 loss: 1.5691\n",
      "Train step 4500 loss: 1.8488\n",
      "Saved model to checkpoint/model-4500\n",
      "Train step 4550 loss: 1.6526\n",
      " Epoch 21 has finished!\n",
      "Train step 4600 loss: 0.9520\n",
      "---------- Evaluate on step 4600 -----------\n",
      "loss on dev set: 2.053073625564575\n",
      "recall on dev set: 0.7477916666666665\n",
      "accuracy on dev set: 0.8082083333333334\n",
      "F score on dev set: 0.7768270650349899\n",
      "Train step 4650 loss: 1.7578\n",
      "Train step 4700 loss: 1.5234\n",
      "Train step 4750 loss: 1.8493\n",
      "Train step 4800 loss: 1.8962\n",
      "---------- Evaluate on step 4800 -----------\n",
      "loss on dev set: 2.0484112119674682\n",
      "recall on dev set: 0.751333333333333\n",
      "accuracy on dev set: 0.8125833333333334\n",
      "F score on dev set: 0.7807589208006962\n",
      " Epoch 22 has finished!\n",
      "Train step 4850 loss: 1.2729\n",
      "Train step 4900 loss: 1.6723\n",
      "Train step 4950 loss: 1.6083\n",
      "Train step 5000 loss: 2.9818\n",
      "---------- Evaluate on step 5000 -----------\n",
      "loss on dev set: 2.0607984495162963\n",
      "recall on dev set: 0.7501041666666666\n",
      "accuracy on dev set: 0.8144583333333334\n",
      "F score on dev set: 0.7809577302405268\n",
      "Saved model to checkpoint/model-5000\n",
      " Epoch 23 has finished!\n",
      "Train step 5050 loss: 1.2784\n",
      "Train step 5100 loss: 1.4283\n",
      "Train step 5150 loss: 1.3039\n",
      "Train step 5200 loss: 1.2725\n",
      "---------- Evaluate on step 5200 -----------\n",
      "loss on dev set: 2.063732237815857\n",
      "recall on dev set: 0.7506249999999999\n",
      "accuracy on dev set: 0.8130833333333334\n",
      "F score on dev set: 0.7806067974100028\n",
      "Train step 5250 loss: 2.1462\n",
      " Epoch 24 has finished!\n",
      "Train step 5300 loss: 1.6330\n",
      "Train step 5350 loss: 1.3640\n",
      "Train step 5400 loss: 1.4415\n",
      "---------- Evaluate on step 5400 -----------\n",
      "loss on dev set: 2.1379819965362548\n",
      "recall on dev set: 0.7566749999999999\n",
      "accuracy on dev set: 0.8222083333333334\n",
      "F score on dev set: 0.7880816492141071\n",
      "Train step 5450 loss: 1.4817\n",
      " Epoch 25 has finished!\n",
      "Train step 5500 loss: 0.8587\n",
      "Saved model to checkpoint/model-5500\n",
      "Train step 5550 loss: 0.9363\n",
      "Train step 5600 loss: 1.5498\n",
      "---------- Evaluate on step 5600 -----------\n",
      "loss on dev set: 2.04636248588562\n",
      "recall on dev set: 0.7564583333333335\n",
      "accuracy on dev set: 0.8160833333333335\n",
      "F score on dev set: 0.785140452911511\n",
      "Train step 5650 loss: 1.5244\n",
      " Epoch 26 has finished!\n",
      "Train step 5700 loss: 1.4301\n",
      "Train step 5750 loss: 1.3338\n",
      "Train step 5800 loss: 2.0829\n",
      "---------- Evaluate on step 5800 -----------\n",
      "loss on dev set: 2.1019260454177857\n",
      "recall on dev set: 0.7718416666666665\n",
      "accuracy on dev set: 0.8347083333333333\n",
      "F score on dev set: 0.8020449673904396\n",
      "Train step 5850 loss: 0.8793\n",
      "Train step 5900 loss: 1.2682\n",
      " Epoch 27 has finished!\n",
      "Train step 5950 loss: 1.8892\n",
      "Train step 6000 loss: 1.6653\n",
      "---------- Evaluate on step 6000 -----------\n",
      "loss on dev set: 2.128469591140747\n",
      "recall on dev set: 0.7667291666666667\n",
      "accuracy on dev set: 0.8275833333333334\n",
      "F score on dev set: 0.7959948623797267\n",
      "Saved model to checkpoint/model-6000\n",
      "Train step 6050 loss: 1.2870\n",
      "Train step 6100 loss: 1.3090\n",
      " Epoch 28 has finished!\n",
      "Train step 6150 loss: 2.0870\n",
      "Train step 6200 loss: 1.4593\n",
      "---------- Evaluate on step 6200 -----------\n",
      "loss on dev set: 2.078633050918579\n",
      "recall on dev set: 0.7777083333333333\n",
      "accuracy on dev set: 0.8370416666666665\n",
      "F score on dev set: 0.806284910374843\n",
      "Train step 6250 loss: 1.2638\n",
      "Train step 6300 loss: 1.5530\n",
      "Train step 6350 loss: 1.5738\n",
      " Epoch 29 has finished!\n",
      "Train step 6400 loss: 2.2445\n",
      "---------- Evaluate on step 6400 -----------\n",
      "loss on dev set: 2.072098355293274\n",
      "recall on dev set: 0.7778333333333332\n",
      "accuracy on dev set: 0.8332916666666667\n",
      "F score on dev set: 0.8046080033792811\n",
      "Train step 6450 loss: 1.2146\n",
      "Train step 6500 loss: 1.2678\n",
      "Saved model to checkpoint/model-6500\n",
      "Train step 6550 loss: 1.3687\n",
      " Epoch 30 has finished!\n"
     ]
    }
   ],
   "source": [
    "evaluate_every = 200\n",
    "checkpoint_every = 500\n",
    "num_batches_per_epoch = int((len(x_train) - 1) / batch_size) + 1\n",
    "for train_batch in train_batches:\n",
    "    x_train_batch, y_train_batch = zip(*train_batch)\n",
    "    train_loss = train_step(x_train_batch, y_train_batch)\n",
    "    current_step = tf.train.global_step(sess, global_step)\n",
    "    if current_step % 50 == 0:\n",
    "        print('Train step %d loss: %.4f' % (current_step, train_loss))\n",
    "    \n",
    "    \"\"\"Step 6.1: evaluate the model with x_dev and y_dev (batch by batch)\"\"\"\n",
    "    if current_step % evaluate_every == 0:\n",
    "        print('---------- Evaluate on step %d -----------' % current_step)\n",
    "        dev_batches = data_helper.batch_iter(list(zip(x_dev, y_dev)), 32, 1)\n",
    "        eval_counter, eval_loss, eval_rec_ts, eval_acc_ts, eval_F_ts = 0, 0.0, 0.0, 0.0, 0.0\n",
    "        for dev_batch in dev_batches:\n",
    "            x_dev_batch, y_dev_batch = zip(*dev_batch)\n",
    "            cur_rec_ts, cur_acc_ts, cur_F_ts, cur_loss, _ = dev_step(x_dev_batch, y_dev_batch)\n",
    "            # update ts scores\n",
    "            eval_counter += 1\n",
    "            eval_rec_ts, eval_acc_ts = eval_rec_ts + cur_rec_ts, eval_acc_ts + cur_acc_ts\n",
    "            eval_loss += cur_loss\n",
    "        # calculate metrics on the whole dev set\n",
    "        eval_loss = float(eval_loss / eval_counter)\n",
    "        eval_rec_ts = float(eval_rec_ts / eval_counter)\n",
    "        eval_acc_ts = float(eval_acc_ts / eval_counter)\n",
    "        eval_F_ts = data_helper.cal_F(eval_rec_ts, eval_acc_ts)\n",
    "\n",
    "        print('loss on dev set: {}'.format(eval_loss))\n",
    "        print('recall on dev set: {}'.format(eval_rec_ts))\n",
    "        print('accuracy on dev set: {}'.format(eval_acc_ts))\n",
    "        print('F score on dev set: {}'.format(eval_F_ts))\n",
    "\n",
    "    \"\"\"Step 6.2: save the model if it is the best based on accuracy on dev set\"\"\"\n",
    "    if current_step % checkpoint_every == 0:\n",
    "        checkpoint_prefix = os.path.join(\"checkpoint\", \"model\")\n",
    "        path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "        print('Saved model to {}'.format(path))\n",
    "    if current_step % num_batches_per_epoch == 0:\n",
    "        current_epoch = current_step // num_batches_per_epoch\n",
    "        print(\" Epoch {} has finished!\".format(current_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          cocoa       1.00      0.78      0.88        18\n",
      "        plywood       0.00      0.00      0.00         0\n",
      "            tea       0.00      0.00      0.00         4\n",
      "         coffee       1.00      0.89      0.94        28\n",
      "            gas       0.00      0.00      0.00        17\n",
      "         lumber       0.00      0.00      0.00         6\n",
      "            gnp       0.94      0.49      0.64        35\n",
      "            wpi       1.00      0.30      0.46        10\n",
      "           earn       0.95      0.95      0.95      1088\n",
      "            oat       0.00      0.00      0.00         6\n",
      "         nickel       0.00      0.00      0.00         1\n",
      "       palm-oil       0.00      0.00      0.00        10\n",
      "            hog       1.00      0.33      0.50         6\n",
      "    instal-debt       0.00      0.00      0.00         1\n",
      "           corn       0.88      0.39      0.54        56\n",
      "         barley       0.00      0.00      0.00        14\n",
      "           lead       0.00      0.00      0.00        14\n",
      "            jet       0.00      0.00      0.00         1\n",
      "         cotton       1.00      0.05      0.10        20\n",
      "       pet-chem       0.00      0.00      0.00        12\n",
      "           ship       0.65      0.31      0.42        89\n",
      "    inventories       0.00      0.00      0.00         0\n",
      "           heat       1.00      0.40      0.57         5\n",
      "            dlr       1.00      0.05      0.09        44\n",
      "           rice       0.00      0.00      0.00        24\n",
      "        soybean       0.00      0.00      0.00        33\n",
      "        austdlr       0.00      0.00      0.00         0\n",
      "       rapeseed       1.00      0.11      0.20         9\n",
      "            yen       0.00      0.00      0.00        14\n",
      "           alum       0.71      0.22      0.33        23\n",
      "            lei       0.00      0.00      0.00         3\n",
      "      groundnut       0.00      0.00      0.00         4\n",
      "           gold       0.91      0.67      0.77        30\n",
      "       interest       0.76      0.35      0.48       133\n",
      "        oilseed       0.50      0.06      0.11        47\n",
      "         silver       0.00      0.00      0.00         8\n",
      "       platinum       0.00      0.00      0.00         7\n",
      "      livestock       0.86      0.25      0.39        24\n",
      "        coconut       0.00      0.00      0.00         2\n",
      "            ipi       0.67      0.17      0.27        12\n",
      "        soy-oil       0.00      0.00      0.00        11\n",
      "            tin       0.00      0.00      0.00        12\n",
      "        sun-oil       0.00      0.00      0.00         2\n",
      "        sunseed       0.00      0.00      0.00         5\n",
      "     iron-steel       0.00      0.00      0.00        14\n",
      "       soy-meal       0.00      0.00      0.00        13\n",
      "   money-supply       0.61      0.32      0.42        34\n",
      "            cpi       0.53      0.29      0.37        28\n",
      "        carcass       0.00      0.00      0.00        18\n",
      "        veg-oil       1.00      0.22      0.36        37\n",
      "         income       0.00      0.00      0.00         7\n",
      "       money-fx       0.66      0.67      0.66       180\n",
      "            bop       1.00      0.07      0.12        30\n",
      "         rubber       1.00      0.17      0.29        12\n",
      "       rape-oil       0.00      0.00      0.00         3\n",
      "            acq       0.71      0.94      0.81       719\n",
      "            dmk       0.00      0.00      0.00         4\n",
      "         retail       0.00      0.00      0.00         2\n",
      "            stg       0.00      0.00      0.00         0\n",
      "           fuel       1.00      0.20      0.33        10\n",
      "          sugar       1.00      0.78      0.88        36\n",
      "          crude       0.76      0.79      0.78       189\n",
      "        housing       0.00      0.00      0.00         4\n",
      "           jobs       1.00      0.24      0.38        21\n",
      "         copper       0.75      0.33      0.46        18\n",
      "       l-cattle       0.00      0.00      0.00         2\n",
      "          trade       0.58      0.80      0.67       117\n",
      "          grain       0.74      0.77      0.75       149\n",
      "      meal-feed       0.00      0.00      0.00        19\n",
      "    coconut-oil       0.00      0.00      0.00         3\n",
      "           zinc       0.00      0.00      0.00        13\n",
      "        sorghum       1.00      0.10      0.18        10\n",
      "         orange       1.00      0.27      0.43        11\n",
      "        nat-gas       1.00      0.13      0.24        30\n",
      "          wheat       0.89      0.70      0.79        71\n",
      "strategic-metal       0.00      0.00      0.00        11\n",
      "       reserves       0.83      0.28      0.42        18\n",
      "\n",
      "    avg / total       0.75      0.68      0.68      3721\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hieuhuynh/Apps/anaconda3/envs/text-cls-py3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/hieuhuynh/Apps/anaconda3/envs/text-cls-py3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Step 7: predict x_test (batch by batch)\"\"\"\n",
    "from sklearn import metrics\n",
    "test_batches = data_helper.batch_iter(list(zip(x_test, y_test)), batch_size=32, num_epochs=1)\n",
    "y_preds, y_test_reform = [], []\n",
    "for test_batch in test_batches:\n",
    "    x_test_batch, y_test_batch = zip(*test_batch)\n",
    "    _, _, _, _, y_pred_batch = dev_step(x_test_batch, y_test_batch)\n",
    "    y_preds.append(y_pred_batch)\n",
    "    y_test_reform.append(y_test_batch)\n",
    "y_preds = np.vstack(y_preds)\n",
    "y_test_reform = np.vstack(y_test_reform)\n",
    "report = metrics.classification_report(y_test_reform, y_preds, target_names=topics_selected)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3016, 77)\n",
      "(3016, 1267) (3016, 77)\n"
     ]
    }
   ],
   "source": [
    "print(y_preds.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}